{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成数据，按顺序进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分组完成，结果已保存到 ./train1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取数据文件\n",
    "data = pd.read_csv(\"./train.csv\")\n",
    "\n",
    "def are_strictly_ordered(user1_scores, user2_scores):\n",
    "    \"\"\"\n",
    "    检查 user1 和 user2 是否满足强排序关系。\n",
    "    \"\"\"\n",
    "    # 找出共同的 item_id\n",
    "    common_items = set(user1_scores[\"item_id\"]).intersection(set(user2_scores[\"item_id\"]))\n",
    "    if not common_items:\n",
    "        return False  # 如果没有共同题目，强排序关系不存在\n",
    "\n",
    "    # 对共同题目进行比较\n",
    "    user1_common_scores = user1_scores[user1_scores[\"item_id\"].isin(common_items)].set_index(\"item_id\")[\"score\"]\n",
    "    user2_common_scores = user2_scores[user2_scores[\"item_id\"].isin(common_items)].set_index(\"item_id\")[\"score\"]\n",
    "\n",
    "    user1_common_scores, user2_common_scores = user1_common_scores.align(user2_common_scores, join=\"inner\")\n",
    "\n",
    "    # 检查是否满足所有分数都 >= 或 <= 的条件\n",
    "    return all(user1_common_scores >= user2_common_scores) or all(user1_common_scores <= user2_common_scores)\n",
    "\n",
    "def assign_groups_with_strict_ordering(data, max_group_size=10):\n",
    "    \"\"\"\n",
    "    按照强排序规则分组，并基于强排序分配组内排名 (fairness_id)。\n",
    "    \"\"\"\n",
    "    user_ids = data[\"user_id\"].unique()\n",
    "    user_scores = {user_id: data[data[\"user_id\"] == user_id][[\"item_id\", \"score\"]] for user_id in user_ids}\n",
    "\n",
    "    assigned_users = set()  # 已分配的用户\n",
    "    groupid_mapping = {user_id: 0 for user_id in user_ids}  # 用户到组的映射\n",
    "    group_id = 1  # 当前组号\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        if user_id in assigned_users:\n",
    "            continue\n",
    "\n",
    "        # 当前组初始化\n",
    "        current_group = [user_id]\n",
    "        assigned_users.add(user_id)\n",
    "\n",
    "        for candidate_id in user_ids:\n",
    "            if candidate_id in assigned_users:\n",
    "                continue\n",
    "\n",
    "            # 检查与组内所有用户的强排序关系\n",
    "            if all(are_strictly_ordered(user_scores[member_id], user_scores[candidate_id]) for member_id in current_group):\n",
    "                # 添加到当前组\n",
    "                current_group.append(candidate_id)\n",
    "                assigned_users.add(candidate_id)\n",
    "\n",
    "            if len(current_group) >= max_group_size:\n",
    "                break\n",
    "\n",
    "        # 分配 group_id\n",
    "        for member_id in current_group:\n",
    "            groupid_mapping[member_id] = group_id\n",
    "\n",
    "        group_id += 1  # 增加组号\n",
    "\n",
    "    return groupid_mapping\n",
    "\n",
    "\n",
    "# 执行分组逻辑\n",
    "groupid_mapping = assign_groups_with_strict_ordering(data)\n",
    "\n",
    "# 添加 group_id 列\n",
    "data[\"group_id\"] = data[\"user_id\"].map(groupid_mapping)\n",
    "\n",
    "# 为每个 user_id 分配唯一的 global fairness_id，从 1 开始递增\n",
    "unique_users = data[[\"user_id\", \"group_id\"]].drop_duplicates().sort_values(by=[\"group_id\", \"user_id\"]).reset_index(drop=True)\n",
    "unique_users[\"fairness_id\"] = range(1, len(unique_users) + 1)\n",
    "\n",
    "# 将 `fairness_id` 映射回原数据\n",
    "data = data.merge(unique_users[[\"user_id\", \"fairness_id\"]], on=\"user_id\", how=\"left\")\n",
    "\n",
    "# 最后按照 fairness_id 排序\n",
    "data = data.sort_values(by=\"fairness_id\").reset_index(drop=True)\n",
    "\n",
    "# 保存结果\n",
    "output_path = \"./train1.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"分组完成，结果已保存到 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建数据集\n",
    "data = pd.read_csv(\"./train1.csv\")\n",
    "\n",
    "# 按 group_id 统计 user_id 的唯一个数\n",
    "group_user_count = data.groupby('group_id')['user_id'].nunique()\n",
    "\n",
    "# 找出 user_id 个数为 0 的 group_id\n",
    "empty_groups = group_user_count[group_user_count == 1].index\n",
    "\n",
    "# 将空组的 group_id 标记为 0\n",
    "data['group_id'] = data['group_id'].apply(lambda x: 0 if x in empty_groups else x)\n",
    "\n",
    "# 保存结果\n",
    "data.to_csv(\"./train2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到 output_with_group.csv 文件中！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 示例数据\n",
    "data = pd.read_csv(\"./train2.csv\")\n",
    "\n",
    "# 按 group_id 聚合出每个组的 user_id 列表\n",
    "group_user_mapping = data.groupby(\"group_id\")[\"user_id\"].unique().to_dict()\n",
    "\n",
    "# 增加一列 get_group，内容为同一组的 user_id 列表\n",
    "data[\"get_group\"] = data[\"group_id\"].map(group_user_mapping)\n",
    "\n",
    "# 对 group_id=0 的行，将 get_group 列设置为空列表 []\n",
    "data.loc[data[\"group_id\"] == 0, \"get_group\"] = data[data[\"group_id\"] == 0].apply(lambda _: [5000], axis=1)\n",
    "\n",
    "# 保存到 CSV 文件\n",
    "data.to_csv(\"./train3.csv\", index=False)\n",
    "\n",
    "print(\"数据已保存到 output_with_group.csv 文件中！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 读取CSV文件并计算每个组的最小fairness_id和组内人数\n",
    "group_info = {}\n",
    "\n",
    "with open('./train3.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        group_id = row['group_id']\n",
    "        fairness_id = int(row['fairness_id'])\n",
    "        group = row['get_group'].strip('[]').split()\n",
    "        group_size = len(group)\n",
    "        \n",
    "        if group_id not in group_info:\n",
    "            group_info[group_id] = {'min_fairness_id': fairness_id, 'group_size': group_size}\n",
    "        else:\n",
    "            group_info[group_id]['min_fairness_id'] = min(group_info[group_id]['min_fairness_id'], fairness_id)\n",
    "\n",
    "# 重新读取CSV文件并添加新列\n",
    "with open('./train3.csv', 'r') as infile, open('./train4.csv', 'w', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames + ['groupindex', 'group_size']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        group_id = row['group_id']\n",
    "        row['groupindex'] = group_info[group_id]['min_fairness_id']\n",
    "        row['group_size'] = group_info[group_id]['group_size']\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# 读取CSV文件并按组存储数据\n",
    "grouped_data = {}\n",
    "\n",
    "with open('./train4.csv', 'r') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        group_id = row['group_id']\n",
    "        if group_id not in grouped_data:\n",
    "            grouped_data[group_id] = []\n",
    "        grouped_data[group_id].append(row)\n",
    "\n",
    "# 对每个组内的数据进行打乱\n",
    "for group_id in grouped_data:\n",
    "    random.shuffle(grouped_data[group_id])\n",
    "\n",
    "# 将打乱后的数据写入新的CSV文件\n",
    "with open('./train5.csv', 'w', newline='') as outfile:\n",
    "    fieldnames = reader.fieldnames\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for group_id in grouped_data:\n",
    "        for row in grouped_data[group_id]:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，结果已保存到 ./trainfinial.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# 读取数据集\n",
    "df = pd.read_csv('./train5.csv')\n",
    "item_df = pd.read_csv('./item.csv')  # 包含 'item_id' 和 'knowledge_code' 列\n",
    "\n",
    "# 创建 item_id 到 knowledge_code 的映射\n",
    "item_to_knowledge = item_df.set_index('item_id')['knowledge_code'].to_dict()\n",
    "\n",
    "# 过滤掉 group_id 为 0 的行\n",
    "df = df[df['group_id'] != 0]\n",
    "\n",
    "# 初始化一个字典，用于存储每个 group_id 的知识点列表\n",
    "group_knowledge = {}\n",
    "\n",
    "# 按 group_id 分组\n",
    "for group_id, group in df.groupby('group_id'):\n",
    "    # 1. 找出 group 内重复的 item_id（公共题目）\n",
    "    item_ids = group['item_id']\n",
    "    duplicated_items = item_ids[item_ids.duplicated()].unique()\n",
    "    \n",
    "    # 2. 根据映射关系获取知识点\n",
    "    knowledge_points = set()\n",
    "    for item_id in duplicated_items:\n",
    "        if item_id in item_to_knowledge:\n",
    "            knowledge_codes = item_to_knowledge[item_id]\n",
    "            # 如果映射的知识点是一个列表，合并所有元素到 set 中去重\n",
    "            if isinstance(knowledge_codes, list):\n",
    "                knowledge_points.update(knowledge_codes)\n",
    "            else:\n",
    "                knowledge_points.add(knowledge_codes)\n",
    "    \n",
    "    # 3. 将知识点列表中的字符串转为真实的列表对象，并进行去重\n",
    "    knowledge_points_cleaned = set()\n",
    "    for kp in knowledge_points:\n",
    "        # 使用 ast.literal_eval 安全地将字符串列表转为真实的 Python 列表\n",
    "        try:\n",
    "            kp_list = ast.literal_eval(kp)\n",
    "            if isinstance(kp_list, list):  # 确保转换为列表\n",
    "                knowledge_points_cleaned.update(kp_list)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # 如果转换失败，跳过该项\n",
    "    \n",
    "    # 4. 对去重后的知识点列表进行排序\n",
    "    group_knowledge[group_id] = sorted(list(knowledge_points_cleaned))\n",
    "\n",
    "# 将计算好的知识点列表映射回原数据集\n",
    "df['common_knowledge'] = df['group_id'].map(group_knowledge)\n",
    "\n",
    "# 保存结果\n",
    "df.to_csv('./trainfinial.csv', index=False)\n",
    "\n",
    "print(\"处理完成，结果已保存到 ./trainfinial.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test集合修正\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载数据集\n",
    "mapping_data = pd.read_csv(\"./trainfinial.csv\")\n",
    "test_data = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "# 构建 user_id 到 fairness_id 的映射\n",
    "user_to_fairness = dict(zip(mapping_data[\"user_id\"], mapping_data[\"fairness_id\"]))\n",
    "\n",
    "# 映射 test 数据集中的 fairness_id\n",
    "test_data[\"fairness_id\"] = test_data[\"user_id\"].map(user_to_fairness)\n",
    "\n",
    "# 检查映射结果\n",
    "#print(test_data)\n",
    "\n",
    "# 保存到文件\n",
    "test_data.to_csv(\"./test_finial.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid集合修正\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载数据集\n",
    "mapping_data = pd.read_csv(\"./trainfinial.csv\")\n",
    "test_data = pd.read_csv(\"./valid.csv\")\n",
    "\n",
    "# 构建 user_id 到 fairness_id 的映射\n",
    "user_to_fairness = dict(zip(mapping_data[\"user_id\"], mapping_data[\"fairness_id\"]))\n",
    "\n",
    "# 映射 test 数据集中的 fairness_id\n",
    "test_data[\"fairness_id\"] = test_data[\"user_id\"].map(user_to_fairness)\n",
    "\n",
    "# 检查映射结果\n",
    "#print(test_data)\n",
    "\n",
    "# 保存到文件\n",
    "test_data.to_csv(\"./valid_finial.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面一段，以修复test集合和valid集合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取数据集\n",
    "train_data = pd.read_csv(\"trainfinial.csv\")\n",
    "test_data = pd.read_csv(\"test_finial.csv\")\n",
    "\n",
    "# 提取 user_id 和 fairness_id 的映射\n",
    "train_mapping = train_data[['user_id', 'fairness_id']].drop_duplicates()\n",
    "test_mapping = test_data[['user_id', 'fairness_id']].drop_duplicates()\n",
    "\n",
    "# 合并映射，查找不一致的条目\n",
    "merged_mapping = pd.merge(\n",
    "    test_mapping, train_mapping, on='user_id', suffixes=('_test', '_train'), how='left'\n",
    ")\n",
    "\n",
    "# 找到与 train 中不一致的条目\n",
    "inconsistent_user_ids = merged_mapping[\n",
    "    (merged_mapping['fairness_id_train'].isna()) |  # 不在 train 中\n",
    "    (merged_mapping['fairness_id_test'] != merged_mapping['fairness_id_train'])  # 映射不同\n",
    "]['user_id']\n",
    "\n",
    "# 从 test 集合中删除不一致的条目\n",
    "filtered_test_data = test_data[~test_data['user_id'].isin(inconsistent_user_ids)]\n",
    "\n",
    "# 保存过滤后的数据集\n",
    "filtered_test_data.to_csv(\"test_finial.csv\", index=False)\n",
    "print(f\"Filtered test data saved to 'test_finial_filtered.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
